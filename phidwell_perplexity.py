#!/usr/bin/env python3"""Φ-Dwell Brain Perplexity Analyzer===================================Treats the brain's eigenmode word sequence as a language and measureshow predictable it is using information-theoretic tools from NLP.What this actually computes (honestly):  - Perplexity: how surprised an n-gram model is by the next eigenmode word.    Lower = more predictable. Higher = more random or novel.  - Cross-perplexity: train on REST, score on TASK (and vice versa).    If task perplexity is HIGH when scored by rest model → task uses    different sequential patterns than rest.  - Masked prediction: hide one word, predict from neighbors.    Accuracy measures how constrained the grammar is.  - Surprise timecourse: per-word surprisal over time, showing when    the brain does something unexpected.  - Vocabulary overlap & conditional entropy breakdown.What this does NOT compute:  - "Thoughts." These are spatial eigenmode configurations, not semantic content.  - "The syntax of the mind." These are transition statistics of dominant    spatial modes. Calling them syntax is a metaphor.Usage:    python phidwell_perplexity.py "physionet.org/files/eegmmidb/1.0.0/" --subjects 5    python phidwell_perplexity.py "path/to/edf_folder/" --subjects 20Requirements:    pip install numpy scipy matplotlib mne"""import numpy as npimport scipy.signalimport scipy.linalgfrom scipy import statsimport matplotlibmatplotlib.use('Agg')import matplotlib.pyplot as pltfrom matplotlib.gridspec import GridSpecfrom collections import Counter, defaultdictimport argparseimport sysimport osimport globimport jsonimport warningswarnings.filterwarnings('ignore')try:    import mne    HAS_MNE = Trueexcept ImportError:    HAS_MNE = False# ═══════════════════════════════════════════════════════════════# CONSTANTS# ═══════════════════════════════════════════════════════════════BANDS = {    'delta': (1, 4), 'theta': (4, 8), 'alpha': (8, 13),    'beta': (13, 30), 'gamma': (30, 50),}BAND_NAMES = list(BANDS.keys())N_MODES = 8MODE_NAMES = ['A-P', 'L-R', 'C-P', 'Diag', 'M5', 'M6', 'M7', 'M8']RUN_CONDITIONS = {    'R01': 'rest_eo', 'R02': 'rest_ec',    'R03': 'exec_lr', 'R04': 'imag_lr',    'R05': 'exec_lr', 'R06': 'imag_lr',    'R07': 'exec_bf', 'R08': 'imag_bf',    'R09': 'exec_lr', 'R10': 'imag_lr',    'R11': 'exec_bf', 'R12': 'imag_bf',    'R13': 'exec_lr', 'R14': 'imag_lr',}ELECTRODE_POS_64 = {    'Fc5.': (-0.59, 0.31), 'Fc3.': (-0.35, 0.31), 'Fc1.': (-0.12, 0.31),    'Fcz.': (0.0, 0.31), 'Fc2.': (0.12, 0.31), 'Fc4.': (0.35, 0.31),    'Fc6.': (0.59, 0.31), 'C5..': (-0.67, 0.0), 'C3..': (-0.40, 0.0),    'C1..': (-0.13, 0.0), 'Cz..': (0.0, 0.0), 'C2..': (0.13, 0.0),    'C4..': (0.40, 0.0), 'C6..': (0.67, 0.0), 'Cp5.': (-0.59, -0.31),    'Cp3.': (-0.35, -0.31), 'Cp1.': (-0.12, -0.31), 'Cpz.': (0.0, -0.31),    'Cp2.': (0.12, -0.31), 'Cp4.': (0.35, -0.31), 'Cp6.': (0.59, -0.31),    'Fp1.': (-0.25, 0.90), 'Fpz.': (0.0, 0.92), 'Fp2.': (0.25, 0.90),    'Af7.': (-0.52, 0.75), 'Af3.': (-0.25, 0.72), 'Afz.': (0.0, 0.72),    'Af4.': (0.25, 0.72), 'Af8.': (0.52, 0.75), 'F7..': (-0.73, 0.52),    'F5..': (-0.55, 0.52), 'F3..': (-0.35, 0.52), 'F1..': (-0.12, 0.52),    'Fz..': (0.0, 0.52), 'F2..': (0.12, 0.52), 'F4..': (0.35, 0.52),    'F6..': (0.55, 0.52), 'F8..': (0.73, 0.52), 'Ft7.': (-0.80, 0.18),    'Ft8.': (0.80, 0.18), 'T7..': (-0.85, 0.0), 'T8..': (0.85, 0.0),    'T9..': (-0.95, 0.0), 'T10.': (0.95, 0.0), 'Tp7.': (-0.80, -0.18),    'Tp8.': (0.80, -0.18), 'P7..': (-0.73, -0.52), 'P5..': (-0.55, -0.52),    'P3..': (-0.35, -0.52), 'P1..': (-0.12, -0.52), 'Pz..': (0.0, -0.52),    'P2..': (0.12, -0.52), 'P4..': (0.35, -0.52), 'P6..': (0.55, -0.52),    'P8..': (0.73, -0.52), 'Po7.': (-0.45, -0.72), 'Po3.': (-0.25, -0.72),    'Poz.': (0.0, -0.72), 'Po4.': (0.25, -0.72), 'Po8.': (0.45, -0.72),    'O1..': (-0.25, -0.87), 'Oz..': (0.0, -0.87), 'O2..': (0.25, -0.87),    'Iz..': (0.0, -0.95),}# ═══════════════════════════════════════════════════════════════# GRAPH LAPLACIAN# ═══════════════════════════════════════════════════════════════def build_graph_laplacian(positions, sigma=0.5):    names = sorted(positions.keys())    N = len(names)    coords = np.array([positions[n] for n in names])    A = np.zeros((N, N))    for i in range(N):        for j in range(i + 1, N):            d = np.sqrt(np.sum((coords[i] - coords[j]) ** 2))            w = np.exp(-d ** 2 / (2 * sigma ** 2))            A[i, j] = A[j, i] = w    D = np.diag(A.sum(axis=1))    L = D - A    vals, vecs = scipy.linalg.eigh(L)    return names, coords, vecs[:, 1:N_MODES + 1], vals[1:N_MODES + 1]def map_channels(raw_names, graph_names):    mapping = {}    lookup = {}    for gn in graph_names:        clean = gn.replace('.', '').lower()        lookup[clean] = gn    for ch in raw_names:        clean = ch.replace('EEG', '').strip().replace('.', '').replace(' ', '').lower()        if clean in lookup:            mapping[ch] = lookup[clean]        else:            for eclean, gname in lookup.items():                if clean.rstrip('.') == eclean or eclean.rstrip('.') == clean:                    mapping[ch] = gname                    break    return mapping# ═══════════════════════════════════════════════════════════════# EEG → WORD SEQUENCE# ═══════════════════════════════════════════════════════════════def eeg_to_words(filepath, graph_names, eigenvecs, word_step_ms=25):    """Convert EDF → eigenmode word sequence."""    if not HAS_MNE:        print("ERROR: mne required. pip install mne")        return None        raw = mne.io.read_raw_edf(filepath, preload=True, verbose='error')    mapping = map_channels(raw.ch_names, graph_names)        if len(mapping) < 10:        return None        sfreq = raw.info['sfreq']    data = raw.get_data()    n_samp = raw.n_times    n_elec = len(graph_names)        phases = np.zeros((5, n_elec, n_samp), dtype=np.complex64)        for bi, band in enumerate(BAND_NAMES):        lo, hi = BANDS[band]        sos = scipy.signal.butter(3, [lo, hi], btype='band', fs=sfreq, output='sos')        for raw_ch, graph_ch in mapping.items():            idx_g = graph_names.index(graph_ch)            idx_r = raw.ch_names.index(raw_ch)            sig = scipy.signal.sosfiltfilt(sos, data[idx_r])            analytic = scipy.signal.hilbert(sig)            phases[bi, idx_g, :] = analytic / (np.abs(analytic) + 1e-9)        coeffs = np.abs(np.tensordot(phases, eigenvecs, axes=([1], [0])))    tokens = np.argmax(coeffs, axis=2)  # (5, n_samp)        step = max(1, int(sfreq * word_step_ms / 1000))    tokens_ds = tokens[:, ::step]        words = [tuple(tokens_ds[:, t]) for t in range(tokens_ds.shape[1])]    return words# ═══════════════════════════════════════════════════════════════# N-GRAM LANGUAGE MODEL# ═══════════════════════════════════════════════════════════════class EigenmodeLanguageModel:    """    Simple n-gram language model over eigenmode word sequences.        Uses Kneser-Ney-like smoothing (add-k) to handle unseen n-grams.    This is deliberately simple — the point is measuring brain dynamics,    not building a state-of-the-art LM.    """        def __init__(self, order=2, smoothing=0.01):        """        order: 1=unigram, 2=bigram, 3=trigram        smoothing: add-k smoothing constant        """        self.order = order        self.smoothing = smoothing        self.ngram_counts = defaultdict(Counter)        self.context_totals = defaultdict(int)        self.vocab = set()        self.unigram_counts = Counter()        self.total_tokens = 0        def train(self, words):        """Train on a word sequence."""        self.vocab.update(words)        self.unigram_counts.update(words)        self.total_tokens += len(words)                for i in range(len(words) - self.order + 1):            ngram = tuple(words[i:i + self.order])            context = ngram[:-1]            target = ngram[-1]            self.ngram_counts[context][target] += 1            self.context_totals[context] += 1        def log_prob(self, word, context):        """Log2 probability of word given context (tuple)."""        if len(context) != self.order - 1:            context = context[-(self.order - 1):]                V = max(len(self.vocab), 1)        k = self.smoothing                if context in self.ngram_counts:            count = self.ngram_counts[context].get(word, 0)            total = self.context_totals[context]            prob = (count + k) / (total + k * V)        else:            # Backoff to unigram            count = self.unigram_counts.get(word, 0)            prob = (count + k) / (self.total_tokens + k * V)                return np.log2(prob + 1e-15)        def sequence_surprisal(self, words):        """Compute per-word surprisal (negative log prob) for a sequence."""        surprisals = []        for i in range(self.order - 1, len(words)):            context = tuple(words[i - self.order + 1:i])            word = words[i]            surprisals.append(-self.log_prob(word, context))        return np.array(surprisals)        def perplexity(self, words):        """Perplexity of the model on a word sequence."""        surprisals = self.sequence_surprisal(words)        if len(surprisals) == 0:            return float('inf')        mean_surprisal = np.mean(surprisals)        return 2.0 ** mean_surprisal        def masked_predict(self, words, mask_idx):        """Predict word at mask_idx from surrounding context."""        start = max(0, mask_idx - self.order + 1)        context = tuple(words[start:mask_idx])                # Find most likely word given context        if context in self.ngram_counts:            return self.ngram_counts[context].most_common(1)[0][0]        else:            return self.unigram_counts.most_common(1)[0][0]# ═══════════════════════════════════════════════════════════════# PERPLEXITY ANALYSIS# ═══════════════════════════════════════════════════════════════def compute_perplexity_analysis(words_rest, words_task):    """    Full perplexity analysis between rest and task conditions.        Trains models on each condition, scores on both → cross-perplexity matrix.    Also computes surprisal timecourse and masked prediction accuracy.    """    results = {}        for order_name, order in [('bigram', 2), ('trigram', 3)]:        # Train models        model_rest = EigenmodeLanguageModel(order=order)        model_rest.train(words_rest)                model_task = EigenmodeLanguageModel(order=order)        model_task.train(words_task)                # Cross-perplexity matrix        pp_rest_on_rest = model_rest.perplexity(words_rest)        pp_rest_on_task = model_rest.perplexity(words_task)        pp_task_on_rest = model_task.perplexity(words_rest)        pp_task_on_task = model_task.perplexity(words_task)                # Surprisal timecourses        surp_rest_self = model_rest.sequence_surprisal(words_rest)        surp_task_self = model_task.sequence_surprisal(words_task)        surp_task_by_rest = model_rest.sequence_surprisal(words_task)        surp_rest_by_task = model_task.sequence_surprisal(words_rest)                # Masked prediction accuracy (sample 500 positions)        n_test = min(500, len(words_rest) - order, len(words_task) - order)                correct_rest = 0        test_indices = np.random.choice(            range(order, len(words_rest)), size=n_test, replace=False        )        for idx in test_indices:            pred = model_rest.masked_predict(words_rest, idx)            if pred == words_rest[idx]:                correct_rest += 1                correct_task = 0        test_indices = np.random.choice(            range(order, len(words_task)), size=n_test, replace=False        )        for idx in test_indices:            pred = model_task.masked_predict(words_task, idx)            if pred == words_task[idx]:                correct_task += 1                # Cross-prediction: rest model predicts task words        correct_cross = 0        test_indices = np.random.choice(            range(order, len(words_task)), size=n_test, replace=False        )        for idx in test_indices:            pred = model_rest.masked_predict(words_task, idx)            if pred == words_task[idx]:                correct_cross += 1                results[order_name] = {            'cross_perplexity': {                'rest→rest': pp_rest_on_rest,                'rest→task': pp_rest_on_task,                'task→rest': pp_task_on_rest,                'task→task': pp_task_on_task,            },            'surprisal': {                'rest_self': surp_rest_self,                'task_self': surp_task_self,                'task_by_rest': surp_task_by_rest,                'rest_by_task': surp_rest_by_task,            },            'masked_accuracy': {                'rest_self': correct_rest / n_test,                'task_self': correct_task / n_test,                'rest→task': correct_cross / n_test,                'n_test': n_test,            },            'vocab_rest': len(set(words_rest)),            'vocab_task': len(set(words_task)),        }        # Surprise spike detection (using bigram model)    surp = results['bigram']['surprisal']['task_by_rest']    mean_surp = np.mean(surp)    std_surp = np.std(surp)    threshold = mean_surp + 2 * std_surp    spike_indices = np.where(surp > threshold)[0]        # Cluster spikes into events (within 4 words = 100ms of each other)    events = []    if len(spike_indices) > 0:        current_event = [spike_indices[0]]        for idx in spike_indices[1:]:            if idx - current_event[-1] <= 4:                current_event.append(idx)            else:                events.append(current_event)                current_event = [idx]        events.append(current_event)        results['surprise_events'] = {        'n_spikes': len(spike_indices),        'n_events': len(events),        'threshold': threshold,        'mean_surprisal': mean_surp,        'std_surprisal': std_surp,        'event_durations_ms': [len(e) * 25 for e in events[:20]],    }        return results# ═══════════════════════════════════════════════════════════════# VOCABULARY STATISTICS# ═══════════════════════════════════════════════════════════════def vocab_statistics(words):    """Basic vocabulary stats for a word sequence."""    counts = Counter(words)    total = len(words)    vocab = len(counts)    probs = np.array(list(counts.values())) / total    entropy = -np.sum(probs * np.log2(probs + 1e-15))        # Zipf's law fit    freqs = np.array(sorted(counts.values(), reverse=True))    ranks = np.arange(1, len(freqs) + 1)    if len(freqs) > 5:        n_fit = min(100, len(freqs))        slope, intercept, r, _, _ = stats.linregress(            np.log(ranks[:n_fit]), np.log(freqs[:n_fit])        )        zipf_alpha = -slope        zipf_r2 = r ** 2    else:        zipf_alpha = 0        zipf_r2 = 0        # Self-transition rate    n_self = sum(1 for a, b in zip(words[:-1], words[1:]) if a == b)    self_rate = n_self / (len(words) - 1) if len(words) > 1 else 0        # Top words    top = counts.most_common(10)        return {        'vocab': vocab,        'total_words': total,        'entropy': entropy,        'zipf_alpha': zipf_alpha,        'zipf_r2': zipf_r2,        'self_rate': self_rate,        'top_words': top,        'ranks': ranks,        'freqs': freqs,    }# ═══════════════════════════════════════════════════════════════# VISUALIZATION# ═══════════════════════════════════════════════════════════════def plot_perplexity_dashboard(subj_id, pp_results, vstats_rest, vstats_task,                               words_rest, words_task, save_path):    """7-panel dashboard for perplexity analysis."""        fig = plt.figure(figsize=(20, 16))    fig.suptitle(f'Φ-Dwell Brain Perplexity — {subj_id}',                 fontsize=16, fontweight='bold', y=0.98)        gs = GridSpec(3, 4, figure=fig, hspace=0.35, wspace=0.35)    colors = {'rest': '#4A90D9', 'task': '#D94A4A', 'cross': '#8B5CF6'}        # ─── Panel 1: Cross-Perplexity Matrix ───    ax1 = fig.add_subplot(gs[0, 0])    bi = pp_results['bigram']    cp = bi['cross_perplexity']    matrix = np.array([        [cp['rest→rest'], cp['rest→task']],        [cp['task→rest'], cp['task→task']]    ])    im = ax1.imshow(matrix, cmap='RdYlBu_r', aspect='auto')    ax1.set_xticks([0, 1])    ax1.set_xticklabels(['Score REST', 'Score TASK'], fontsize=8)    ax1.set_yticks([0, 1])    ax1.set_yticklabels(['Train REST', 'Train TASK'], fontsize=8)    for i in range(2):        for j in range(2):            ax1.text(j, i, f'{matrix[i, j]:.1f}', ha='center', va='center',                     fontsize=12, fontweight='bold',                     color='white' if matrix[i, j] > np.mean(matrix) else 'black')    ax1.set_title('Cross-Perplexity (Bigram)', fontsize=10, fontweight='bold')    plt.colorbar(im, ax=ax1, fraction=0.046)        # ─── Panel 2: Cross-Perplexity Matrix (Trigram) ───    ax2 = fig.add_subplot(gs[0, 1])    tri = pp_results['trigram']    cp3 = tri['cross_perplexity']    matrix3 = np.array([        [cp3['rest→rest'], cp3['rest→task']],        [cp3['task→rest'], cp3['task→task']]    ])    im2 = ax2.imshow(matrix3, cmap='RdYlBu_r', aspect='auto')    ax2.set_xticks([0, 1])    ax2.set_xticklabels(['Score REST', 'Score TASK'], fontsize=8)    ax2.set_yticks([0, 1])    ax2.set_yticklabels(['Train REST', 'Train TASK'], fontsize=8)    for i in range(2):        for j in range(2):            ax2.text(j, i, f'{matrix3[i, j]:.1f}', ha='center', va='center',                     fontsize=12, fontweight='bold',                     color='white' if matrix3[i, j] > np.mean(matrix3) else 'black')    ax2.set_title('Cross-Perplexity (Trigram)', fontsize=10, fontweight='bold')    plt.colorbar(im2, ax=ax2, fraction=0.046)        # ─── Panel 3: Masked Prediction Accuracy ───    ax3 = fig.add_subplot(gs[0, 2])    bi_acc = bi['masked_accuracy']    tri_acc = tri['masked_accuracy']        categories = ['Rest→Rest', 'Task→Task', 'Rest→Task']    bi_vals = [bi_acc['rest_self'], bi_acc['task_self'], bi_acc['rest→task']]    tri_vals = [tri_acc['rest_self'], tri_acc['task_self'], tri_acc['rest→task']]        x = np.arange(len(categories))    width = 0.35    ax3.bar(x - width/2, bi_vals, width, label='Bigram', color=colors['rest'], alpha=0.8)    ax3.bar(x + width/2, tri_vals, width, label='Trigram', color=colors['task'], alpha=0.8)    ax3.set_xticks(x)    ax3.set_xticklabels(categories, fontsize=8)    ax3.set_ylabel('Accuracy', fontsize=9)    ax3.set_title('Masked Prediction', fontsize=10, fontweight='bold')    ax3.legend(fontsize=8)    ax3.set_ylim(0, max(max(bi_vals), max(tri_vals)) * 1.3)        for i, (bv, tv) in enumerate(zip(bi_vals, tri_vals)):        ax3.text(i - width/2, bv + 0.01, f'{bv:.2f}', ha='center', fontsize=7)        ax3.text(i + width/2, tv + 0.01, f'{tv:.2f}', ha='center', fontsize=7)        # ─── Panel 4: Key Metrics Summary ───    ax4 = fig.add_subplot(gs[0, 3])    ax4.axis('off')        se = pp_results['surprise_events']        summary = [        ('Metric', 'REST', 'TASK'),        ('─' * 12, '─' * 8, '─' * 8),        ('Vocabulary', f"{vstats_rest['vocab']}", f"{vstats_task['vocab']}"),        ('Entropy (bits)', f"{vstats_rest['entropy']:.2f}", f"{vstats_task['entropy']:.2f}"),        ('Self-trans rate', f"{vstats_rest['self_rate']:.3f}", f"{vstats_task['self_rate']:.3f}"),        ('Zipf α', f"{vstats_rest['zipf_alpha']:.2f}", f"{vstats_task['zipf_alpha']:.2f}"),        ('Bigram PP (self)', f"{cp['rest→rest']:.1f}", f"{cp['task→task']:.1f}"),        ('Bigram PP (cross)', f"—", f"{cp['rest→task']:.1f}"),        ('Trigram PP (self)', f"{cp3['rest→rest']:.1f}", f"{cp3['task→task']:.1f}"),        ('Surprise events', '—', f"{se['n_events']}"),    ]        y = 0.95    for row in summary:        if row[0].startswith('─'):            ax4.text(0.05, y, '─' * 40, fontsize=7, family='monospace',                     transform=ax4.transAxes, color='gray')        else:            ax4.text(0.05, y, f'{row[0]:<16}', fontsize=8, family='monospace',                     transform=ax4.transAxes, fontweight='bold' if y > 0.9 else 'normal')            ax4.text(0.55, y, f'{row[1]:>8}', fontsize=8, family='monospace',                     transform=ax4.transAxes, color=colors['rest'])            ax4.text(0.78, y, f'{row[2]:>8}', fontsize=8, family='monospace',                     transform=ax4.transAxes, color=colors['task'])        y -= 0.095    ax4.set_title('Summary', fontsize=10, fontweight='bold')        # ─── Panel 5: Surprisal Timecourse (Task scored by Rest model) ───    ax5 = fig.add_subplot(gs[1, :3])    surp = bi['surprisal']['task_by_rest']    time_ms = np.arange(len(surp)) * 25 / 1000  # seconds        ax5.plot(time_ms, surp, color=colors['task'], alpha=0.4, linewidth=0.5)        # Smoothed version    if len(surp) > 40:        kernel = np.ones(40) / 40  # 1-second window        smoothed = np.convolve(surp, kernel, mode='same')        ax5.plot(time_ms, smoothed, color=colors['task'], linewidth=2, label='1s avg')        # Threshold line    ax5.axhline(se['threshold'], color='orange', linestyle='--', alpha=0.7,                label=f'Surprise threshold (μ+2σ)')        # Mean line    ax5.axhline(se['mean_surprisal'], color='gray', linestyle=':', alpha=0.5)        ax5.set_xlabel('Time (seconds)', fontsize=9)    ax5.set_ylabel('Surprisal (bits)', fontsize=9)    ax5.set_title('Task Surprisal (scored by REST model) — spikes = unexpected brain states',                  fontsize=10, fontweight='bold')    ax5.legend(fontsize=8, loc='upper right')        # ─── Panel 6: Surprisal comparison self vs cross ───    ax_cmp = fig.add_subplot(gs[1, 3])    surp_self = bi['surprisal']['task_self']    surp_cross = bi['surprisal']['task_by_rest']        ax_cmp.hist(surp_self, bins=50, alpha=0.6, color=colors['task'],                label='Task→Task', density=True)    ax_cmp.hist(surp_cross, bins=50, alpha=0.6, color=colors['cross'],                label='Rest→Task', density=True)    ax_cmp.set_xlabel('Surprisal (bits)', fontsize=9)    ax_cmp.set_ylabel('Density', fontsize=9)    ax_cmp.set_title('Surprisal Distribution', fontsize=10, fontweight='bold')    ax_cmp.legend(fontsize=8)        # ─── Panel 7: Zipf's Law Comparison ───    ax6 = fig.add_subplot(gs[2, 0])    ax6.loglog(vstats_rest['ranks'], vstats_rest['freqs'], 'o-',               color=colors['rest'], markersize=2, alpha=0.7, label='REST')    ax6.loglog(vstats_task['ranks'], vstats_task['freqs'], 'o-',               color=colors['task'], markersize=2, alpha=0.7, label='TASK')    ax6.set_xlabel('Rank', fontsize=9)    ax6.set_ylabel('Frequency', fontsize=9)    ax6.set_title("Zipf's Law", fontsize=10, fontweight='bold')    ax6.legend(fontsize=8)    ax6.text(0.05, 0.05,             f"α_rest={vstats_rest['zipf_alpha']:.2f} (R²={vstats_rest['zipf_r2']:.2f})\n"             f"α_task={vstats_task['zipf_alpha']:.2f} (R²={vstats_task['zipf_r2']:.2f})",             transform=ax6.transAxes, fontsize=7, family='monospace',             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))        # ─── Panel 8: Perplexity Bar Chart (all conditions) ───    ax7 = fig.add_subplot(gs[2, 1])    labels = ['R→R', 'R→T', 'T→R', 'T→T']    bi_pp = [cp['rest→rest'], cp['rest→task'], cp['task→rest'], cp['task→task']]    tri_pp = [cp3['rest→rest'], cp3['rest→task'], cp3['task→rest'], cp3['task→task']]        x = np.arange(len(labels))    width = 0.35    bars1 = ax7.bar(x - width/2, bi_pp, width, label='Bigram', color=colors['rest'], alpha=0.8)    bars2 = ax7.bar(x + width/2, tri_pp, width, label='Trigram', color=colors['task'], alpha=0.8)    ax7.set_xticks(x)    ax7.set_xticklabels(labels, fontsize=9)    ax7.set_ylabel('Perplexity', fontsize=9)    ax7.set_title('Perplexity: Train→Score', fontsize=10, fontweight='bold')    ax7.legend(fontsize=8)        for bar, val in zip(bars1, bi_pp):        ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,                 f'{val:.0f}', ha='center', fontsize=7)    for bar, val in zip(bars2, tri_pp):        ax7.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,                 f'{val:.0f}', ha='center', fontsize=7)        # ─── Panel 9: Top eigenmode words comparison ───    ax8 = fig.add_subplot(gs[2, 2:])    ax8.axis('off')        # Build comparison table    top_rest = vstats_rest['top_words'][:8]    top_task = vstats_task['top_words'][:8]        header = f"{'Rank':<5} {'REST Top Words':<22} {'%':>6}   {'TASK Top Words':<22} {'%':>6}"    ax8.text(0.02, 0.95, header, fontsize=8, family='monospace',             fontweight='bold', transform=ax8.transAxes)    ax8.text(0.02, 0.89, '─' * 70, fontsize=7, family='monospace',             transform=ax8.transAxes, color='gray')        y = 0.82    for i in range(max(len(top_rest), len(top_task))):        rw = top_rest[i] if i < len(top_rest) else (None, 0)        tw = top_task[i] if i < len(top_task) else (None, 0)                r_str = ''.join(str(x+1) for x in rw[0]) if rw[0] else '—'        r_pct = rw[1] / vstats_rest['total_words'] * 100 if rw[0] else 0        t_str = ''.join(str(x+1) for x in tw[0]) if tw[0] else '—'        t_pct = tw[1] / vstats_task['total_words'] * 100 if tw[0] else 0                # Decode mode names        def decode_word(w):            if w is None:                return ''            return ' '.join(MODE_NAMES[x] for x in w)                line = f"  {i+1:<4} {r_str:<8} {decode_word(rw[0]):<13} {r_pct:>5.1f}%   "        line += f"{t_str:<8} {decode_word(tw[0]):<13} {t_pct:>5.1f}%"                ax8.text(0.02, y, line, fontsize=7.5, family='monospace',                 transform=ax8.transAxes)        y -= 0.09        ax8.set_title('Top Eigenmode Words', fontsize=10, fontweight='bold')        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')    plt.close()    print(f"  Saved: {save_path}")# ═══════════════════════════════════════════════════════════════# CROSS-SUBJECT SUMMARY# ═══════════════════════════════════════════════════════════════def plot_cross_subject_summary(all_results, save_path):    """Summary across all subjects."""        fig = plt.figure(figsize=(18, 10))    fig.suptitle('Φ-Dwell Brain Perplexity — Cross-Subject Summary',                 fontsize=16, fontweight='bold', y=0.98)        gs = GridSpec(2, 3, figure=fig, hspace=0.35, wspace=0.3)        subjects = sorted(all_results.keys())    n = len(subjects)    x = np.arange(n)        colors_r = '#4A90D9'    colors_t = '#D94A4A'    colors_c = '#8B5CF6'        # ─── Panel 1: Perplexity rest vs task (bigram) ───    ax1 = fig.add_subplot(gs[0, 0])    pp_rest = [all_results[s]['bigram_pp_rest'] for s in subjects]    pp_task = [all_results[s]['bigram_pp_task'] for s in subjects]    pp_cross = [all_results[s]['bigram_pp_cross'] for s in subjects]        width = 0.25    ax1.bar(x - width, pp_rest, width, label='REST→REST', color=colors_r, alpha=0.8)    ax1.bar(x, pp_task, width, label='TASK→TASK', color=colors_t, alpha=0.8)    ax1.bar(x + width, pp_cross, width, label='REST→TASK', color=colors_c, alpha=0.8)    ax1.set_xticks(x)    ax1.set_xticklabels(subjects, fontsize=8, rotation=45)    ax1.set_ylabel('Perplexity', fontsize=9)    ax1.set_title('Bigram Perplexity', fontsize=11, fontweight='bold')    ax1.legend(fontsize=7)        # ─── Panel 2: Vocabulary expansion ───    ax2 = fig.add_subplot(gs[0, 1])    v_rest = [all_results[s]['vocab_rest'] for s in subjects]    v_task = [all_results[s]['vocab_task'] for s in subjects]        ax2.bar(x - 0.2, v_rest, 0.4, label='REST', color=colors_r, alpha=0.8)    ax2.bar(x + 0.2, v_task, 0.4, label='TASK', color=colors_t, alpha=0.8)    ax2.set_xticks(x)    ax2.set_xticklabels(subjects, fontsize=8, rotation=45)    ax2.set_ylabel('Vocabulary Size', fontsize=9)    ax2.set_title('Vocabulary Expansion', fontsize=11, fontweight='bold')    ax2.legend(fontsize=8)        for i, (r, t) in enumerate(zip(v_rest, v_task)):        ratio = t / r if r > 0 else 0        ax2.text(i, max(r, t) + 50, f'{ratio:.1f}×', ha='center', fontsize=7,                 fontweight='bold')        # ─── Panel 3: Masked prediction accuracy ───    ax3 = fig.add_subplot(gs[0, 2])    acc_rest = [all_results[s]['masked_acc_rest'] for s in subjects]    acc_task = [all_results[s]['masked_acc_task'] for s in subjects]    acc_cross = [all_results[s]['masked_acc_cross'] for s in subjects]        ax3.bar(x - width, acc_rest, width, label='REST→REST', color=colors_r, alpha=0.8)    ax3.bar(x, acc_task, width, label='TASK→TASK', color=colors_t, alpha=0.8)    ax3.bar(x + width, acc_cross, width, label='REST→TASK', color=colors_c, alpha=0.8)    ax3.set_xticks(x)    ax3.set_xticklabels(subjects, fontsize=8, rotation=45)    ax3.set_ylabel('Accuracy', fontsize=9)    ax3.set_title('Masked Prediction Accuracy', fontsize=11, fontweight='bold')    ax3.legend(fontsize=7)        # ─── Panel 4: Entropy shift ───    ax4 = fig.add_subplot(gs[1, 0])    e_rest = [all_results[s]['entropy_rest'] for s in subjects]    e_task = [all_results[s]['entropy_task'] for s in subjects]        ax4.bar(x - 0.2, e_rest, 0.4, label='REST', color=colors_r, alpha=0.8)    ax4.bar(x + 0.2, e_task, 0.4, label='TASK', color=colors_t, alpha=0.8)    ax4.set_xticks(x)    ax4.set_xticklabels(subjects, fontsize=8, rotation=45)    ax4.set_ylabel('Shannon Entropy (bits)', fontsize=9)    ax4.set_title('Entropy: Rest vs Task', fontsize=11, fontweight='bold')    ax4.legend(fontsize=8)        # ─── Panel 5: Zipf exponents ───    ax5 = fig.add_subplot(gs[1, 1])    z_rest = [all_results[s]['zipf_rest'] for s in subjects]    z_task = [all_results[s]['zipf_task'] for s in subjects]        ax5.bar(x - 0.2, z_rest, 0.4, label='REST', color=colors_r, alpha=0.8)    ax5.bar(x + 0.2, z_task, 0.4, label='TASK', color=colors_t, alpha=0.8)    ax5.set_xticks(x)    ax5.set_xticklabels(subjects, fontsize=8, rotation=45)    ax5.set_ylabel('Zipf Exponent α', fontsize=9)    ax5.set_title("Zipf's Law Exponent", fontsize=11, fontweight='bold')    ax5.legend(fontsize=8)    ax5.axhline(1.0, color='gray', linestyle=':', alpha=0.5, label='Zipf α=1')        # ─── Panel 6: Surprise event count ───    ax6 = fig.add_subplot(gs[1, 2])    se = [all_results[s]['surprise_events'] for s in subjects]    ax6.bar(x, se, color=colors_c, alpha=0.8)    ax6.set_xticks(x)    ax6.set_xticklabels(subjects, fontsize=8, rotation=45)    ax6.set_ylabel('Count', fontsize=9)    ax6.set_title('Surprise Events (Task by Rest model)', fontsize=11, fontweight='bold')        plt.savefig(save_path, dpi=150, bbox_inches='tight', facecolor='white')    plt.close()    print(f"  Saved: {save_path}")# ═══════════════════════════════════════════════════════════════# MAIN# ═══════════════════════════════════════════════════════════════def find_subject_dirs(base_path, n_subjects=5):    """Find subject directories in PhysioNet structure."""    candidates = sorted(glob.glob(os.path.join(base_path, 'S[0-9][0-9][0-9]')))    if not candidates:        candidates = sorted(glob.glob(os.path.join(base_path, 'files', 'eegmmidb',                                                     '1.0.0', 'S[0-9][0-9][0-9]')))    if not candidates:        if os.path.isdir(base_path) and glob.glob(os.path.join(base_path, '*.edf')):            return [base_path]    return candidates[:n_subjects]def get_condition_files(subject_dir):    """Get REST and TASK files for a subject."""    all_edfs = sorted(glob.glob(os.path.join(subject_dir, '*.edf')))        rest_files = []    task_files = []        for f in all_edfs:        basename = os.path.basename(f)        for run_key, cond in RUN_CONDITIONS.items():            if run_key in basename:                if cond.startswith('rest'):                    rest_files.append(f)                elif cond.startswith('imag'):                    task_files.append(f)                break        return rest_files, task_filesdef main():    parser = argparse.ArgumentParser(description='Φ-Dwell Brain Perplexity Analyzer')    parser.add_argument('path', help='Path to PhysioNet eegmmidb directory')    parser.add_argument('--subjects', type=int, default=5, help='Number of subjects')    args = parser.parse_args()        print("╔══════════════════════════════════════════════════╗")    print("║   Φ-DWELL BRAIN PERPLEXITY ANALYZER             ║")    print("║   Eigenmode Language Model                      ║")    print("╚══════════════════════════════════════════════════╝")    print()        # Build eigenmodes    graph_names, coords, eigenvecs, eigenvals = build_graph_laplacian(ELECTRODE_POS_64)    print(f"Graph Laplacian: {len(graph_names)} electrodes, {N_MODES} modes")    print(f"Eigenvalues: {eigenvals.round(2)}")        # Find subjects    subject_dirs = find_subject_dirs(args.path, args.subjects)    if not subject_dirs:        print(f"ERROR: No subject directories found in {args.path}")        sys.exit(1)        print(f"Found {len(subject_dirs)} subjects\n")        all_cross_results = {}    all_results_json = {}        for sdir in subject_dirs:        subj_id = os.path.basename(sdir)        print(f"\n{'═' * 60}")        print(f"  SUBJECT: {subj_id}")        print(f"{'═' * 60}")                rest_files, task_files = get_condition_files(sdir)                if not rest_files or not task_files:            print(f"  Skipping {subj_id}: missing REST or TASK files")            continue                print(f"  REST files: {len(rest_files)}, TASK files: {len(task_files)}")                # Tokenize        words_rest = []        for f in rest_files:            result = eeg_to_words(f, graph_names, eigenvecs)            if result:                words_rest.extend(result)                print(f"    {os.path.basename(f)}: {len(result)} words")                words_task = []        for f in task_files:            result = eeg_to_words(f, graph_names, eigenvecs)            if result:                words_task.extend(result)                print(f"    {os.path.basename(f)}: {len(result)} words")                if not words_rest or not words_task:            print(f"  Skipping {subj_id}: tokenization failed")            continue                print(f"\n  Total: REST={len(words_rest)} words, TASK={len(words_task)} words")                # Vocabulary stats        vstats_rest = vocab_statistics(words_rest)        vstats_task = vocab_statistics(words_task)                print(f"  Vocabulary: REST={vstats_rest['vocab']}, TASK={vstats_task['vocab']} "              f"({vstats_task['vocab']/vstats_rest['vocab']:.1f}×)")        print(f"  Entropy: REST={vstats_rest['entropy']:.2f}, TASK={vstats_task['entropy']:.2f} bits")        print(f"  Self-transition: REST={vstats_rest['self_rate']:.3f}, TASK={vstats_task['self_rate']:.3f}")        print(f"  Zipf α: REST={vstats_rest['zipf_alpha']:.2f}, TASK={vstats_task['zipf_alpha']:.2f}")                # Perplexity analysis        print(f"\n  Computing perplexity...")        pp_results = compute_perplexity_analysis(words_rest, words_task)                bi = pp_results['bigram']        tri = pp_results['trigram']                print(f"\n  ┌─────────────────────────────────────────┐")        print(f"  │  BIGRAM CROSS-PERPLEXITY                 │")        print(f"  │  REST→REST: {bi['cross_perplexity']['rest→rest']:>8.1f}                │")        print(f"  │  REST→TASK: {bi['cross_perplexity']['rest→task']:>8.1f}  ← key metric   │")        print(f"  │  TASK→REST: {bi['cross_perplexity']['task→rest']:>8.1f}                │")        print(f"  │  TASK→TASK: {bi['cross_perplexity']['task→task']:>8.1f}                │")        print(f"  └─────────────────────────────────────────┘")                print(f"\n  ┌─────────────────────────────────────────┐")        print(f"  │  TRIGRAM CROSS-PERPLEXITY                │")        print(f"  │  REST→REST: {tri['cross_perplexity']['rest→rest']:>8.1f}                │")        print(f"  │  REST→TASK: {tri['cross_perplexity']['rest→task']:>8.1f}  ← key metric   │")        print(f"  │  TASK→REST: {tri['cross_perplexity']['task→rest']:>8.1f}                │")        print(f"  │  TASK→TASK: {tri['cross_perplexity']['task→task']:>8.1f}                │")        print(f"  └─────────────────────────────────────────┘")                print(f"\n  Masked Prediction Accuracy (bigram):")        print(f"    REST→REST: {bi['masked_accuracy']['rest_self']:.3f}")        print(f"    TASK→TASK: {bi['masked_accuracy']['task_self']:.3f}")        print(f"    REST→TASK: {bi['masked_accuracy']['rest→task']:.3f}")                se = pp_results['surprise_events']        print(f"\n  Surprise Events (task scored by rest model):")        print(f"    Threshold: {se['threshold']:.2f} bits (μ+2σ)")        print(f"    Events: {se['n_events']} ({se['n_spikes']} individual spikes)")        if se['event_durations_ms']:            print(f"    Duration range: {min(se['event_durations_ms'])}-{max(se['event_durations_ms'])} ms")                # Plot        fig_path = os.path.join(os.path.dirname(sdir) if os.path.dirname(sdir) else '.',                                f'phidwell_perplexity_{subj_id}.png')        plot_perplexity_dashboard(subj_id, pp_results, vstats_rest, vstats_task,                                  words_rest, words_task, fig_path)                # Collect cross-subject results        all_cross_results[subj_id] = {            'bigram_pp_rest': bi['cross_perplexity']['rest→rest'],            'bigram_pp_task': bi['cross_perplexity']['task→task'],            'bigram_pp_cross': bi['cross_perplexity']['rest→task'],            'vocab_rest': vstats_rest['vocab'],            'vocab_task': vstats_task['vocab'],            'masked_acc_rest': bi['masked_accuracy']['rest_self'],            'masked_acc_task': bi['masked_accuracy']['task_self'],            'masked_acc_cross': bi['masked_accuracy']['rest→task'],            'entropy_rest': vstats_rest['entropy'],            'entropy_task': vstats_task['entropy'],            'zipf_rest': vstats_rest['zipf_alpha'],            'zipf_task': vstats_task['zipf_alpha'],            'surprise_events': se['n_events'],        }                all_results_json[subj_id] = {            'vocab_rest': vstats_rest['vocab'],            'vocab_task': vstats_task['vocab'],            'vocab_ratio': vstats_task['vocab'] / vstats_rest['vocab'],            'entropy_rest': vstats_rest['entropy'],            'entropy_task': vstats_task['entropy'],            'self_rate_rest': vstats_rest['self_rate'],            'self_rate_task': vstats_task['self_rate'],            'zipf_alpha_rest': vstats_rest['zipf_alpha'],            'zipf_alpha_task': vstats_task['zipf_alpha'],            'bigram_perplexity': {                'rest_self': bi['cross_perplexity']['rest→rest'],                'task_self': bi['cross_perplexity']['task→task'],                'rest_to_task': bi['cross_perplexity']['rest→task'],                'task_to_rest': bi['cross_perplexity']['task→rest'],            },            'trigram_perplexity': {                'rest_self': tri['cross_perplexity']['rest→rest'],                'task_self': tri['cross_perplexity']['task→task'],                'rest_to_task': tri['cross_perplexity']['rest→task'],                'task_to_rest': tri['cross_perplexity']['task→rest'],            },            'masked_accuracy_bigram': {                'rest_self': bi['masked_accuracy']['rest_self'],                'task_self': bi['masked_accuracy']['task_self'],                'cross': bi['masked_accuracy']['rest→task'],            },            'surprise_events': se['n_events'],            'surprise_spikes': se['n_spikes'],        }        # Cross-subject summary    if len(all_cross_results) > 1:        print(f"\n\n{'═' * 60}")        print(f"  CROSS-SUBJECT SUMMARY ({len(all_cross_results)} subjects)")        print(f"{'═' * 60}")                subjects = sorted(all_cross_results.keys())                # Key findings        pp_rest_vals = [all_cross_results[s]['bigram_pp_rest'] for s in subjects]        pp_task_vals = [all_cross_results[s]['bigram_pp_task'] for s in subjects]        pp_cross_vals = [all_cross_results[s]['bigram_pp_cross'] for s in subjects]                print(f"\n  Bigram Perplexity:")        print(f"    REST→REST: {np.mean(pp_rest_vals):.1f} ± {np.std(pp_rest_vals):.1f}")        print(f"    TASK→TASK: {np.mean(pp_task_vals):.1f} ± {np.std(pp_task_vals):.1f}")        print(f"    REST→TASK: {np.mean(pp_cross_vals):.1f} ± {np.std(pp_cross_vals):.1f}")                # Is cross-perplexity consistently higher?        n_higher = sum(1 for r, c in zip(pp_task_vals, pp_cross_vals) if c > r)        print(f"\n  Cross > Self for TASK: {n_higher}/{len(subjects)} subjects")                if len(subjects) >= 3:            t_stat, p_val = stats.ttest_rel(pp_cross_vals, pp_task_vals)            print(f"  Paired t-test (cross vs self): t={t_stat:.2f}, p={p_val:.4f}")                # Vocabulary        v_rest = [all_cross_results[s]['vocab_rest'] for s in subjects]        v_task = [all_cross_results[s]['vocab_task'] for s in subjects]        ratios = [t/r for r, t in zip(v_rest, v_task)]        print(f"\n  Vocabulary Ratio (task/rest): {np.mean(ratios):.2f} ± {np.std(ratios):.2f}")        print(f"    All subjects > 1.0: {all(r > 1.0 for r in ratios)}")                # Masked accuracy        acc_cross = [all_cross_results[s]['masked_acc_cross'] for s in subjects]        acc_self = [all_cross_results[s]['masked_acc_task'] for s in subjects]        print(f"\n  Masked Prediction (task):")        print(f"    Self:  {np.mean(acc_self):.3f} ± {np.std(acc_self):.3f}")        print(f"    Cross: {np.mean(acc_cross):.3f} ± {np.std(acc_cross):.3f}")                # Plot summary        summary_path = os.path.join(            os.path.dirname(subject_dirs[0]) if os.path.dirname(subject_dirs[0]) else '.',            'phidwell_perplexity_summary.png'        )        plot_cross_subject_summary(all_cross_results, summary_path)        # Save JSON    json_path = os.path.join(        os.path.dirname(subject_dirs[0]) if os.path.dirname(subject_dirs[0]) else '.',        'phidwell_perplexity_all.json'    )    with open(json_path, 'w') as f:        json.dump(all_results_json, f, indent=2)    print(f"\n  Saved: {json_path}")        print(f"\n{'═' * 60}")    print(f"  INTERPRETATION GUIDE")    print(f"{'═' * 60}")    print(f"""  PERPLEXITY measures how surprised the model is by the next word.    REST→REST: How predictable rest is to itself.    Lower = more repetitive eigenmode cycling.      TASK→TASK: How predictable task is to itself.    Higher than REST = task dynamics are more variable.      REST→TASK: How well rest patterns predict task patterns.    MUCH higher than TASK→TASK = task uses genuinely different    sequential structure, not just more variety.      The GAP between REST→TASK and TASK→TASK is the key metric.  A large gap means the brain during task follows transition  rules that don't exist during rest. A small gap means task  is just noisier rest.      MASKED PREDICTION accuracy measures how constrained the grammar is.  High accuracy = knowing the previous word(s) strongly determines  the next word. Low accuracy = the brain is less deterministic.      SURPRISE EVENTS are moments where the task brain does something  the rest model finds extremely unlikely. These are candidate  moments of novel neural computation — configurations the resting  brain would almost never produce.""")if __name__ == '__main__':    main()